The goal of my project was to explore the world of Neural Networks and Machine Learning. Initially I wanted to build a Neural Network trained to process cancer images and can diagnosis and predict images that are ran through the saved model. However, that proved to be more challenging than expected as the images for medical images are a very specific format and the fact that a single patient doesn't only have a single image for their scans, instead it's a collections of pictures representing the 3D structure of a given organ. Furthermore it was challenging finding online datasets where the malignant tumors and benign are labeled and contained within the same dataset. 

Instead, I wanted to build a foundation for this eventual future endeavour by creating a neural network that instead processes something simplier, like being able to recognize handwritten numbers and correctly predict what number they represent. The folder contains two main programs written in python mostly using the PIL and tensorflow library. mnist.py creates, trains, and saves the neural network model and then predict.py recovers that model and uses it to predict what number inserted images represent.

mnist.py first pulls the mnist dataset from the tensorflow library and downloads it into a tmp file in the current directory, one_hot=True is to make it clearer to the machine that the numbers aren't suppose to be greater than one another (i.e. don't interpret 5 as greater than 4), instead the numbers are represented in an array of binary to avoid this confusion. In this case 5 = 0000010000 and 4 = 0000100000. Next, the components of the neural network are created. data is a placeholder for where the batches of mnist data will be inserted when the session of tensorflow is ran. Weights and biases are necessary components of the Neural network as in each layer the data send to the nodes are multiplied by the weights and added by the bias. weight is for the output layer (last layer where the hypothesis is created) and the hl_weight is for the hidden layer, a layer between the input layer and the output layer where the data is further processed. When declaring these datatypes, it is important to make sure the dimensions match. For example, the input layer outputs to 784 nodes, because the image is 28x28, representing 784 pixels in total. The next layer, the hidden layer, must be able to received these 784 layers, and output to a matching layer of nodes of the next layer, therefore it is 784,500. Finally the output layer is 500,10 and 10 representing the number of classes, in this case 0-9 representing each digit. 

Next I define how each layer is ran, as mentioned above, the data in each layer is multiplied by the weight and added by the bias. That data is then passed through an activation function such as "relu" to check whether or not it should be fired and sent to the next layer. Finally in the output layer, the data is processed through the softmax function to output probability of each class being the correct answer and collectively adding up to 1. It basically represents the output data between 0 and 1, effectively turning it into a probability. 

After the neural network has been created, y is declared, representing the correct label of the data. The cost function is then defined, finding the reduced mean of cross entropy between the predicted label (hypothesis) versus the actual label (y). The cost function represents how far the hypothesis is from the correct answer. The next step is to define an optimizer that minimizes this distance, bringing the hypothesis to a better line of fit to the actual values. There are many optimizers including gradient descent optimizer, but adamoptimizer seemed to work best when I tried the different optimizers. Saver is declared and will later be ran in the session to save the model.

Finally the training is ready to start. First the epoch, or cycles of how many times the neural network is ran (forward propagation + backpropagation). Each epoch is when the optimizer goes back and alters the weight and biases of each layer to better fit the graph representing the actual values. The session is ran and first all the variables has to be initialized before they can be used. It is cleaner to use the with as structure because sessions need to be closed after usage. A loops is used to train the model, dividing the epochs up into the total epoch number and each data sample into each batch size. The batch_x and batch_y are fed into the placeholders and the optimizer and cost functions are ran. The loss of the cost funtion is documented in the epoch_loss, representing the cost function in each of the epoch. Finally. the accuracy is calculated finding the mean of the max value of correct and comparing it to the max value of y as a float and the model is saved. 

Next, the predict.py recreates the model and declares the same components of the neural network for the model to be recovered into. The only difference is that the components are given empty values, or "zeros". The session is ran, restoring the model, then feeding the img data into data through the neural network and returning the prediction. Before this is ran, the function prepare_img takes the file name from the command line argument and processes it incase the dimensions aren't 28x28, because the model was trained to recognize pictures of that dimension. If a picture of another dimension was inserted to the neural network for it to predict, it would cause overfitting because the machine would think that the different dimension were a feature to be considered, leading it to an inaccurate prediction such as (bigger image = 5). The pre_img resizes the image, then takes the px values and inserts it into a blank 28x28 white canvas to replicate the mnist dataset. The data is represented in the pixel values between 0 and 1, 1 representing black and 0 representing 0 as in the mnist dataset. Finally, a main function is called, first retreiving and processing the image, then finally, running the data of that image through the predict function and printing the number that it returns. 



